#!/bin/bash
# Rendered by Terraform/templatefile() — exports populated variables and then runs the installer logic
set -euo pipefail
exec > >(tee /var/log/cloud-init-k3s-server.log | logger -t k3s-user-data -s 2>/dev/console) 2>&1

# --- Terraform-injected values ---
export T_K3S_VERSION="${T_K3S_VERSION}"
export T_K3S_TOKEN="${T_K3S_TOKEN}"
export T_DB_USER="${T_DB_USER}"
export T_DB_NAME_DEV="${T_DB_NAME_DEV}"
export T_DB_NAME_PROD="${T_DB_NAME_PROD}"
export T_DB_SERVICE_NAME_DEV="${T_DB_SERVICE_NAME_DEV}"
export T_DB_SERVICE_NAME_PROD="${T_DB_SERVICE_NAME_PROD}"
export T_MANIFESTS_REPO_URL="${T_MANIFESTS_REPO_URL}"
export T_EXPECTED_NODE_COUNT="${T_EXPECTED_NODE_COUNT}"
export T_PRIVATE_LB_IP="${T_PRIVATE_LB_IP}"

# --- Begin: original k3s-install-server.sh logic (merged) ---

# K3s SERVER install, tooling, secret generation, ingress-nginx install,
# and Argo CD bootstrapping.

# (set -euo pipefail and exec already set above)

install_base_tools() {
  echo "Installing base packages..."
  apt-get update -y || true
  apt-get install -y curl jq git || true
}

get_private_ip() {
  echo "Fetching instance private IP..."
  # Try OCI metadata first
  PRIVATE_IP="$(curl -s -H "Authorization: Bearer Oracle" http://169.254.169.254/opc/v2/vnics/ \
    | jq -r '.[0].privateIp' 2>/dev/null | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' || true)"
  # Fallback via routing table
  if [ -z "$${PRIVATE_IP:-}" ]; then
    PRIVATE_IP="$(ip -4 route get 1.1.1.1 | awk '{for(i=1;i<=NF;i++) if($i=="src"){print $(i+1); exit}}' || true)"
  fi
  if [ -z "$${PRIVATE_IP:-}" ]; then
    echo "❌ Failed to determine private IP."
    exit 1
  fi
  echo "✅ Instance private IP is $PRIVATE_IP"
}

install_k3s_server() {
  echo "Installing K3s server..."

  # Make sure nothing blocks k3s ports
  systemctl disable --now firewalld 2>/dev/null || true
  ufw disable 2>/dev/null || true

  # k3s server flags (explicit binds + SANs)
  local PARAMS="--write-kubeconfig-mode 644 \
    --node-ip $PRIVATE_IP \
    --advertise-address $PRIVATE_IP \
    --disable traefik \
    --tls-san $PRIVATE_IP \
    --tls-san $T_PRIVATE_LB_IP \
    --kube-apiserver-arg=bind-address=0.0.0.0 \
    --kube-apiserver-arg=advertise-address=$PRIVATE_IP \
    --https-listen-port=6443 \
    --kubelet-arg=register-with-taints=node-role.kubernetes.io/control-plane=true:NoSchedule"

  export INSTALL_K3S_EXEC="$PARAMS"
  export K3S_TOKEN="$T_K3S_TOKEN"
  export INSTALL_K3S_VERSION="$T_K3S_VERSION"
  curl -sfL https://get.k3s.io | sh -

  echo "Waiting for K3s server node to be Ready..."
  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  # Consider the node ready when at least one node reports Ready
  until /usr/local/bin/kubectl get nodes --no-headers 2>/dev/null \
      | awk '{print $2}' | grep -Eq '^Ready(,SchedulingDisabled)?$'; do
    echo "Waiting for a Ready control-plane node..."
    sleep 5
  done
  echo "✅ K3s server node is Ready."

  echo "Probing API endpoints for health..."
  set +e
  # Local apiserver
  curl -k --connect-timeout 5 -sS "https://127.0.0.1:6443/healthz" && echo "Local API /healthz: OK" || echo "⚠️ Local /healthz probe failed"
  # Node IP
  curl -k --connect-timeout 5 -sS "https://$PRIVATE_IP:6443/healthz" && echo "Node IP /healthz: OK" || echo "⚠️ $PRIVATE_IP /healthz probe failed"
  # Private LB IP (will return once backends attach and healthchecks pass)
  curl -k --connect-timeout 5 -sS "https://$T_PRIVATE_LB_IP:6443/healthz" && echo "Private LB /healthz: OK" || echo "⚠️ Private LB /healthz probe failed (may be OK until backends register)"
  set -e
}

wait_for_all_nodes() {
  echo "Waiting for all $T_EXPECTED_NODE_COUNT nodes to join and become Ready..."
  local timeout=900
  local start_time; start_time=$(date +%s)
  while true; do
    local ready_nodes
    ready_nodes=$(/usr/local/bin/kubectl get nodes --no-headers 2>/dev/null \
      | awk '{print $2}' | grep -Ec '^Ready(,SchedulingDisabled)?$' || true)
    if [ "$ready_nodes" -eq "$T_EXPECTED_NODE_COUNT" ]; then
      echo "✅ All $T_EXPECTED_NODE_COUNT nodes are Ready. Proceeding."
      break
    fi
    local elapsed_time=$(( $(date +%s) - start_time ))
    if [ "$elapsed_time" -gt "$timeout" ]; then
      echo "⚠️ Timed out waiting for all nodes. Proceeding with addons anyway."
      /usr/local/bin/kubectl get nodes || true
      break
    fi
    echo "($elapsed_time/$timeout s) $ready_nodes/$T_EXPECTED_NODE_COUNT Ready. Waiting..."
    sleep 15
  done
}

install_helm() {
  if ! command -v helm &> /dev/null; then
    echo "Installing Helm..."
    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    chmod 700 get_helm.sh
    ./get_helm.sh
    rm get_helm.sh
  fi
}

install_ingress_nginx() {
  echo "Installing ingress-nginx via Helm (DaemonSet + NodePorts 30080/30443)..."
  helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx || true
  helm repo update

  if ! /usr/local/bin/kubectl get namespace ingress-nginx &> /dev/null; then
    /usr/local/bin/kubectl create namespace ingress-nginx
  fi

  helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
    --namespace ingress-nginx \
    --set controller.kind=DaemonSet \
    --set controller.service.type=NodePort \
    --set controller.service.nodePorts.http=30080 \
    --set controller.service.nodePorts.https=30443 \
    --set controller.service.externalTrafficPolicy=Local \
    --set controller.nodeSelector.role=application \
    --set controller.ingressClassResource.name=nginx \
    --set controller.ingressClassByName=true \
    --wait --timeout=5m

  echo "Waiting for ingress-nginx controller rollout..."
  /usr/local/bin/kubectl -n ingress-nginx rollout status ds/ingress-nginx-controller --timeout=5m || true
}

install_argo_cd() {
  echo "Installing Argo CD..."
  if ! /usr/local/bin/kubectl get namespace argocd &> /dev/null; then
    /usr/local/bin/kubectl create namespace argocd
  fi
  /usr/local/bin/kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

  local tolerations_json='[
    {"op":"add","path":"/spec/template/spec/tolerations","value":[
      {"key":"node-role.kubernetes.io/control-plane","operator":"Exists","effect":"NoSchedule"}
    ]}
  ]'

  for d in argocd-server argocd-repo-server argocd-dex-server; do
    /usr/local/bin/kubectl -n argocd patch deployment "$d" --type='json' -p="$${tolerations_json}" || true
  done
  /usr/local/bin/kubectl -n argocd patch statefulset argocd-application-controller --type='json' -p="$${tolerations_json}" || true

  echo "Waiting for Argo CD components to be available..."
  /usr/local/bin/kubectl -n argocd wait --for=condition=Available deployment/argocd-server --timeout=5m || true
  /usr/local/bin/kubectl -n argocd wait --for=condition=Available deployment/argocd-repo-server --timeout=5m || true
  /usr/local/bin/kubectl -n argocd wait --for=condition=Available deployment/argocd-dex-server --timeout=5m || true
  /usr/local/bin/kubectl -n argocd rollout status statefulset/argocd-application-controller --timeout=5m || true
}

generate_secrets_and_credentials() {
  echo "Generating credentials and Kubernetes secrets..."
  DB_PASSWORD=$(LC_ALL=C tr -dc 'A-Za-z0-9' < /dev/urandom | head -c 32)
  ARGO_PASSWORD="$(/usr/local/bin/kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d || true)"

  cat << EOF > /root/credentials.txt
# --- Argo CD Admin Credentials ---
Username: admin
Password: $${ARGO_PASSWORD}

# --- PostgreSQL Database Credentials ---
Username: ${T_DB_USER}
Password: $${DB_PASSWORD}
EOF
  chmod 600 /root/credentials.txt
  echo "Credentials saved to /root/credentials.txt"

  for ns in default development; do
    if ! /usr/local/bin/kubectl get namespace "$ns" &> /dev/null; then
      /usr/local/bin/kubectl create namespace "$ns"
    fi
    /usr/local/bin/kubectl -n "$ns" create secret generic postgres-credentials \
      --from-literal=POSTGRES_USER="${T_DB_USER}" \
      --from-literal=POSTGRES_PASSWORD="$${DB_PASSWORD}" \
      --dry-run=client -o yaml | /usr/local/bin/kubectl apply -f -
  done

  DB_URI_DEV="postgresql://${T_DB_USER}:$${DB_PASSWORD}@${T_DB_SERVICE_NAME_DEV}-client.development.svc.cluster.local:5432/${T_DB_NAME_DEV}"
  /usr/local/bin/kubectl -n development create secret generic backend-db-connection \
    --from-literal=DB_URI="${DB_URI_DEV}" \
    --dry-run=client -o yaml | /usr/local/bin/kubectl apply -f -

  DB_URI_PROD="postgresql://${T_DB_USER}:$${DB_PASSWORD}@${T_DB_SERVICE_NAME_PROD}-client.default.svc.cluster.local:5432/${T_DB_NAME_PROD}"
  /usr/local/bin/kubectl -n default create secret generic backend-db-connection \
    --from-literal=DB_URI="${DB_URI_PROD}" \
    --dry-run=client -o yaml | /usr/local/bin/kubectl apply -f -
}

bootstrap_argocd_apps() {
  echo "Bootstrapping Argo CD with applications from manifest repo..."
  if [ ! -d "/tmp/manifests" ]; then
    git clone "${T_MANIFESTS_REPO_URL}" /tmp/manifests
  else
    echo "Manifests directory /tmp/manifests already exists. Skipping clone."
  fi

  [ -f /tmp/manifests/clusters/dev/apps/project.yaml ] && /usr/local/bin/kubectl apply -f /tmp/manifests/clusters/dev/apps/project.yaml --server-side || true
  [ -f /tmp/manifests/clusters/dev/apps/stack.yaml ]   && /usr/local/bin/kubectl apply -f /tmp/manifests/clusters/dev/apps/stack.yaml --server-side || true
  [ -f /tmp/manifests/clusters/prod/apps/project.yaml ] && /usr/local/bin/kubectl apply -f /tmp/manifests/clusters/prod/apps/project.yaml --server-side || true
  [ -f /tmp/manifests/clusters/prod/apps/stack.yaml ]   && /usr/local/bin/kubectl apply -f /tmp/manifests/clusters/prod/apps/stack.yaml --server-side
}

# Final main() sequence (same as original)
main() {
  install_base_tools
  get_private_ip
  install_k3s_server
  wait_for_all_nodes
  install_helm
  install_ingress_nginx
  install_argo_cd
  generate_secrets_and_credentials
  bootstrap_argocd_apps
}

main "$@"
